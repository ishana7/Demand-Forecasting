# -*- coding: utf-8 -*-
"""Demand Forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x1YkXtnNYjPcjlL137IcM1Mn_lyCXFf3
"""

!pip install pyspark

!pip install prophet

"""**SAVE BEFORE STARTING!!!**"""

from pyspark.sql import SparkSession
from pyspark import SparkConf, SparkContext
spark = SparkSession.builder.appName("Demand Forecasting").getOrCreate()

train = spark.read.csv('train_file.csv', header=True)

items = spark.read.csv('items.csv', header=True)

store = spark.read.csv('stores.csv', header=True)
transactions = spark.read.csv('transactions.csv', header=True)

holidays_events = spark.read.csv('holidays_events.csv', header=True)

holidays_events = holidays_events.withColumnRenamed('type','holiday_type')

transactions = transactions.withColumnRenamed('date','tdate')

store.show()

transactions.show()

import pyspark.sql.functions as f
items_train = train.alias("d1").join(items.alias("d2"), f.col("d1.item_nbr") == f.col("d2.item_nbr"), "left").drop(train.item_nbr)
store_itemTrain = items_train.alias("d1").join(store.alias("d2"), f.col("d1.store_nbr") == f.col("d2.store_nbr"), "left").drop(items_train.store_nbr)

# Joining Train data with items 
items_train.show()

#Train + Items + Store
store_itemTrain.printSchema()

store_itemTrain.show()

# from pyspark.sql.functions import *
# date_field = store_itemTrain.select('date')
# r=to_date(col("date_field"),"MM-dd-yyyy"))

store_itemTrain.show(2)

# from pyspark.sql.functions import *
# store_itemTrain.select("date",to_date("date","yyyy-MM-dd")).show()

# store_itemTrain=store_itemTrain.select(col("id"),col("unit_sales"),col("item_nbr"),col("family"),col("family"),col("class"),col("perishable"),col("date"),col('store_nbr'),col('city'),col('state'),col('type'),col('cluster'),to_date(col("date"),"").alias("store_itemTrain_date"))

store_itemTrain.printSchema()

df3 = store_itemTrain.alias("st").join(transactions.alias("t"),(f.col("st.store_nbr") == f.col("t.store_nbr")) & (f.col("st.date") == f.col("t.tdate")), "left").drop(store_itemTrain.store_nbr)

# df3 = store_itemTrain.alias("st").join(transactions.alias("t"),f.col("st.store_nbr") == f.col("t.store_nbr"), "left").drop(store_itemTrain.store_nbr)

df3=df3.drop('tdate')

df3.show()

oil = spark.read.csv('oil.csv', header=True)

oil.show()

df4 = df3.join(oil, ['date'], 'left')

df4.show()

df4.printSchema()

import pyspark.sql.functions as func
df4 = df4.withColumn("dcoilwtico", func.round(df4["dcoilwtico"]).cast('integer'))

df4 = df4.na.fill(value=0,subset=["dcoilwtico"])

df4.show()

df5 = df4.join(holidays_events, ['date'], 'left')

df5.show()

df4 = df4.drop('id')

#dropping id
df5=df5.drop('id')

df5.printSchema()

from pyspark.sql.types import DateType,IntegerType
df5 = df5.withColumn("transferred",df5["transferred"].cast(IntegerType()))

df5.show(2)

df5 = df5.withColumn("date", df5["date"].cast(DateType()))

df5 = df5.withColumn("unit_sales", df5["unit_sales"].cast(IntegerType()))

df5 = df5.na.fill(value=0,subset=["unit_sales"])

df5.printSchema()

from pyspark.ml.feature import StringIndexer, OneHotEncoder

# final_df.show(5)

from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler

from pyspark.ml.feature import OneHotEncoder

# Spark Pipeline
# cat_features = ['family', 'city', 'state', 'locale','locale_name','type']
# num_features = ['date','item_nbr','item_nbr','class','perishable','store_nbr','transactions','dcoilwtico','transferred']

cat_features = ['family', 'city', 'state','type']
num_features = ['item_nbr','item_nbr','class','perishable','store_nbr','transactions','dcoilwtico']
label = 'unit_sales'

# Pipeline Stages List
stages = []

# Loop for StringIndexer and OHE for Categorical Variables
for features in cat_features:
    
    # Index Categorical Features
    string_indexer = StringIndexer(inputCol=features, outputCol=features + "_index")
    string_indexer=string_indexer.setHandleInvalid("skip")
    
    #One Hot Encode Categorical Features
    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()],
                                     outputCols=[features + "_class_vec"])
    # Append Pipeline Stages
    stages += [string_indexer, encoder]
    
# Index Label Feature
label_str_index =  StringIndexer(inputCol=label, outputCol="label_index")

# Assemble or Concat the Categorical Features and Numeric Features
assembler_inputs = [feature + "_class_vec" for feature in cat_features] + num_features

assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features") 

stages += [label_str_index, assembler]

stages



from pyspark.ml import Pipeline

# Set Pipeline
pipeline = Pipeline(stages=stages)

# Fit Pipeline to Data
pipeline_model = pipeline.fit(df5)

# Transform Data using Fitted Pipeline
df_transform = pipeline_model.transform(df5)

df_transform.limit(5).toPandas()

"""Linear Regression"""

from pyspark.ml.regression import LinearRegression
lr = LinearRegression(featuresCol = 'features', labelCol='label_index', maxIter=10, regParam=0.3, elasticNetParam=0.8)
lr_model = lr.fit(train_data)

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

lr_predictions = lr_model.transform(test_data)
lr_predictions.select("prediction","label_index","features").show(5)
from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="label_index",metricName="r2")
print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(lr_predictions))



"""Time Series Forecasting - Analysis + Facebook Prophet Model

"""

from pyspark.sql.types import DateType,TimestampType
df4 = df4.withColumn("date", df4["date"].cast(TimestampType()))
df4 = df4.withColumn("unit_sales", func.round(df4["unit_sales"]).cast("integer"))

df4 = df4.withColumn("item_nbr", func.round(df4["item_nbr"]).cast("integer"))
df4 = df4.withColumn("store_nbr", func.round(df4["store_nbr"]).cast("integer"))
df4 = df4.withColumn("transactions", func.round(df4["transactions"]).cast("integer"))
df4 = df4.withColumn("cluster", func.round(df4["cluster"]).cast("integer"))
df4 = df4.withColumn("perishable", func.round(df4["perishable"]).cast("integer"))
df4 = df4.withColumn("class", func.round(df4["class"]).cast("integer"))

df4.printSchema()

from pyspark.ml.feature import StringIndexer, OneHotEncoder
indexer = StringIndexer(inputCol='family', outputCol='family_t').fit(df4)
indexed_df = indexer.transform(df4)

indexer = StringIndexer(inputCol='city', outputCol='city_t').fit(indexed_df)
indexed_df1 = indexer.transform(indexed_df)

indexer = StringIndexer(inputCol='state', outputCol='state_t').fit(indexed_df1)
final_df = indexer.transform(indexed_df1)

indexer = StringIndexer(inputCol='type', outputCol='type_t').fit(final_df)
final_df = indexer.transform(final_df)

final_df.show()

final_df.printSchema()

cols = ['id', 'family', 'city', 'state']
final_df =final_df.drop('id', 'family', 'city', 'state', 'type')

final_df1 = final_df.withColumn("date", final_df["date"].cast("string"))

assembler=VectorAssembler().setInputCols(['item_nbr','class','family_t','type_t', 'store_nbr','perishable', 'state_t', 'city_t', 'dcoilwtico', 'transactions']).setOutputCol('features')

output_12 = assembler.transform(final_df1)

output_12.show()



pandasDF = final_df.toPandas()

pandasDF

item_df = pandasDF.set_index('date')

item_df.drop(['id', 'family', 'city', 'state'], axis=1, inplace=True)

item_df

import matplotlib.pyplot as plt
item_df.query('store_nbr == 25')[['unit_sales']].plot()
plt.show()

item_df.query('store_nbr == 2')[['unit_sales']].plot()
plt.show()

final_df.show()

final_df.select(['store_nbr']).groupby('store_nbr').agg({'store_nbr': 'count'}).show()

final_df.createOrReplaceTempView("unit_sales")
spark.sql("select store_nbr, count(*) from unit_sales group by store_nbr order by store_nbr").show()

sql = "SELECT store_nbr, date as ds, sum(unit_sales) as y FROM unit_sales GROUP BY store_nbr, ds ORDER BY store_nbr, ds"
spark.sql(sql).show()

store_partition = (spark.sql(sql).repartition(spark.sparkContext.defaultParallelism, ['store_nbr'])).cache()
store_partition.explain()

from pyspark.sql.types import *
result_schema = StructType([
                  StructField('ds', TimestampType()),
                  StructField('store_nbr', IntegerType()),
                  StructField('y', DoubleType()),
                  StructField('yhat', DoubleType()),
                  StructField('yhat_upper', DoubleType()),
                  StructField('yhat_lower', DoubleType())
])

from pyspark.sql.functions import pandas_udf, PandasUDFType
from prophet import Prophet
@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)
def forecast_sales(store_pd):
  model = Prophet(interval_width=0.95, seasonality_mode= 'multiplicative', weekly_seasonality=True, yearly_seasonality=True)
  model.fit(store_pd)
  future_pd = model.make_future_dataframe(periods=5, freq='w')
  forecast_pd = model.predict(future_pd)
  f_pd = forecast_pd[['ds', 'yhat', 'yhat_upper', 'yhat_lower']].set_index('ds')
  st_pd = store_pd[['ds', 'store_nbr', 'y']].set_index('ds')
  result_pd = f_pd.join(st_pd, how='left')
  result_pd.reset_index(level=0, inplace=True)
  result_pd['store_nbr'] = store_pd['store_nbr'].iloc[0]
  return result_pd[['ds', 'store_nbr', 'y', 'yhat', 'yhat_upper', 'yhat_lower']]

from pyspark.sql.functions import current_date
results = (store_partition.groupby('store_nbr').apply(forecast_sales))
results.cache()
results.show()

results.coalesce(1)
print(results.count())
results.createOrReplaceTempView('forecasted')
spark.sql("SELECT store_nbr, count(*) FROM  forecasted GROUP BY store_nbr").show()

"""ROLLING WINDOW AVERAGE"""

rdf = final_df.withColumn('rolling_average', f.avg("unit_sales").over(Window.partitionBy(f.window("date", "7 days"))))

rdf.select('rolling_average').where('store_nbr == 48').show()



"""Decision Tree Regressor"""

train, test = final_df.randomSplit([0.8,0.2],1234)

assembler=VectorAssembler().setInputCols(['unit_sales','item_nbr','class','family_t','type_t', 'item_nbr']).setOutputCol('features')
train_a1 = assembler.transform(train)
train_b=train_a1.select("features",train_a1.unit_sales.alias('label'))
test_a = assembler.transform(test)
test_b = test_a.select('features', test_a.unit_sales.alias('label'))

from pyspark.ml.regression import DecisionTreeRegressor
dt1 = DecisionTreeRegressor(labelCol="label", featuresCol="features")
model1 = dt1.fit(train_b)
test_dt = model1.transform(test_b)
test_dt.show()

evaluator = RegressionEvaluator()
print(" -------------- R2 Score -----------------")
print("R2 :", evaluator.evaluate(test_dt,
{evaluator.metricName: "r2"})
)
print(" -------------- RMSE Score -----------------")
print("RMSE :",evaluator.evaluate(test_dt,
{evaluator.metricName: "rmse"})
)
print(" -------------- MAE Score -----------------")
print("MAE :",evaluator.evaluate(test_dt,
{evaluator.metricName: "mae"})
)



#Random Forest 
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator

# Create an initial RandomForest model.
rf = RandomForestRegressor(labelCol="label", featuresCol="features")

# Evaluate model
rfevaluator = RegressionEvaluator(predictionCol="prediction", labelCol="label", metricName="rmse")

# Create ParamGrid for Cross Validation
rfparamGrid = (ParamGridBuilder()
             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])
               .addGrid(rf.maxDepth, [2, 5, 10])
             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])
              #  .addGrid(rf.maxBins, [5, 10, 20])
             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])
               .addGrid(rf.numTrees, [5, 15, 30])
             .build())

# Create 5-fold CrossValidator
rfcv = CrossValidator(estimator = rf,
                      estimatorParamMaps = rfparamGrid,
                      evaluator = rfevaluator,
                      numFolds = 6)

# Run cross validations.
rfcvModel = rfcv.fit(train_b)

# print(rfcvModel)

# Use test set here so we can measure the accuracy of our model on new data
rfpredictions = rfcvModel.transform(test_b)

# cvModel uses the best model found from the Cross Validation
# Evaluate best model
print('RMSE:', rfevaluator.evaluate(rfpredictions))

rfpredictions.show()



test_data = spark.read.csv('test.csv', header=True)

test_data.show()

test_data.printSchema()

test_data = test_data.withColumn("item_nbr", func.round(test_data["item_nbr"]).cast("integer"))
test_data = test_data.withColumn("store_nbr", func.round(test_data["store_nbr"]).cast("integer"))

assembler=VectorAssembler().setInputCols(['item_nbr', 'store_nbr']).setOutputCol('features')
test_a = assembler.transform(test_data)
test_b1 = test_a.select('features')

dt = DecisionTreeRegressor(labelCol="label", featuresCol="features")
model = dt.fit(train_b)
test_dt1 = model.transform(test_b1)



